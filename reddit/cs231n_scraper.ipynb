{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reddit api\n",
    "\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download method\n",
    "import requests\n",
    "\n",
    "#other\n",
    "import pprint\n",
    "import re\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "# File management\n",
    "import os, os.path\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloadImage(imageUrl, localFileName):\n",
    "    response = requests.get(imageUrl)\n",
    "#     print(imageUrl)\n",
    "#     if response.status_code == 200:\n",
    "#         print('Downloading %s...' % (localFileName))\n",
    "    with open(localFileName, 'wb') as fo:\n",
    "        for chunk in response.iter_content(4096):\n",
    "            fo.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/8032642/\n",
    "# how-to-obtain-image-size-using-standard-python-class-without-using-external-lib/20380514#20380514\n",
    "import struct\n",
    "import imghdr\n",
    "\n",
    "def test_jpeg(h, f):\n",
    "    # SOI APP2 + ICC_PROFILE\n",
    "    if h[0:4] == '\\xff\\xd8\\xff\\xe2' and h[6:17] == b'ICC_PROFILE':\n",
    "#         print \"A\"\n",
    "        return 'jpeg'\n",
    "    # SOI APP14 + Adobe\n",
    "    if h[0:4] == '\\xff\\xd8\\xff\\xee' and h[6:11] == b'Adobe':\n",
    "        return 'jpeg'\n",
    "    # SOI DQT\n",
    "    if h[0:4] == '\\xff\\xd8\\xff\\xdb':\n",
    "        return 'jpeg'\n",
    "imghdr.tests.append(test_jpeg)\n",
    "\n",
    "def get_image_size(fname):\n",
    "    '''Determine the image type of fhandle and return its size.\n",
    "    from draco'''\n",
    "    with open(fname, 'rb') as fhandle:\n",
    "        head = fhandle.read(24)\n",
    "        if len(head) != 24:\n",
    "            return\n",
    "        what = imghdr.what(None, head)\n",
    "        if what == 'png':\n",
    "            check = struct.unpack('>i', head[4:8])[0]\n",
    "            if check != 0x0d0a1a0a:\n",
    "                return\n",
    "            width, height = struct.unpack('>ii', head[16:24])\n",
    "        elif what == 'gif':\n",
    "            width, height = struct.unpack('<HH', head[6:10])\n",
    "        elif what == 'jpeg':\n",
    "            try:\n",
    "                fhandle.seek(0) # Read 0xff next\n",
    "                size = 2\n",
    "                ftype = 0\n",
    "                while not 0xc0 <= ftype <= 0xcf or ftype in (0xc4, 0xc8, 0xcc):\n",
    "                    fhandle.seek(size, 1)\n",
    "                    byte = fhandle.read(1)\n",
    "                    while ord(byte) == 0xff:\n",
    "                        byte = fhandle.read(1)\n",
    "                    ftype = ord(byte)\n",
    "                    size = struct.unpack('>H', fhandle.read(2))[0] - 2\n",
    "                # We are at a SOFn block\n",
    "                fhandle.seek(1, 1)  # Skip `precision' byte.\n",
    "                height, width = struct.unpack('>HH', fhandle.read(4))\n",
    "            except Exception: #IGNORE:W0703\n",
    "                return\n",
    "        else:\n",
    "            return\n",
    "        return width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_caption(comment):\n",
    "    return re.split('[.!?\\n]',comment)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='NLIznGitXU82bA',\n",
    "                     client_secret='in6SUevIRf3dLYHqu2YFH8xYAq8',\n",
    "                     password='willmarkandmatt',\n",
    "                     user_agent='script2',\n",
    "                     username='231ntest1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231ntest1\n"
     ]
    }
   ],
   "source": [
    "print(reddit.user.me())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(reddit.read_only)  # Output: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Private Internet Access, a VPN provider, takes out a full page ad in The New York Time calling out 50 senators.\n",
      "Thanks, Obama.\n",
      "This is Shelia Fredrick, a flight attendant. She noticed a terrified girl accompanied by an older man. She left a note in the bathroom on which the victim wrote that she needed help. The police was alerted & the girl was saved from a human trafficker. We should honor our heroes.\n",
      "And here it is: Likely the last sketch I'll ever post on Reddit. It's been a wild ride, guys and girls. Thanks for everything.\n",
      "This image is now illegal in Russia.\n",
      "Should have been Bernie\n",
      "I've been photoshopping my kid into marginally dangerous situations. Nothing unbelievable, but enough to make people think \"Wait, did he..?\"\n",
      "I wander the streets of Toronto at night, looking for cinematic moments. This is what I found...\n",
      "Don't forget about this\n",
      "First and last day of 1st grade- she kicked cancers ass!\n"
     ]
    }
   ],
   "source": [
    "for submission in reddit.subreddit('pics').top(\"all\",limit=10):\n",
    "    print(submission.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ' yes', \" it's me\", '']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subName = \"aww\"\n",
    "substats ={\n",
    "    \"title\" : \"rarepup\",\n",
    "    \"comments\" : [\"wow\", \"such rare\", \"so pup\"]\n",
    "}\n",
    "json.dumps(substats)\n",
    "a = \"hello? yes. it's me!\"\n",
    "re.split(\"[?.!]\", a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from r/rarepuppers...\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "\tRetrieved 491 images from r/rarepuppers.\n",
      "\tRetrieved 2941 captions from r/rarepuppers.\n",
      "Extracting data from r/aww...\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "\tRetrieved 305 images from r/aww.\n",
      "\tRetrieved 1830 captions from r/aww.\n",
      "Extracting data from r/eyebleach...\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "\tRetrieved 218 images from r/eyebleach.\n",
      "\tRetrieved 1307 captions from r/eyebleach.\n",
      "Retrieved 1014 total images.\n",
      "Retrieved 6078 total captions\n",
      "Done. (5598.24039412 s)\n"
     ]
    }
   ],
   "source": [
    "# Extract comment + image pairs from subreddit\n",
    "\n",
    "info = {\n",
    "    \"year\" : 2017,\n",
    "    \"version\" : 0,\n",
    "    \"description\" : \"CS231N Toy Dataset\",\n",
    "    \"contributor\" : \"Mathieu Rolfo, Mark Nishimura, William Clary\",\n",
    "    \"url\" : \"\",\n",
    "    \"date_created\" : str(date.today()),\n",
    "}\n",
    "\n",
    "unlicensed_license = {\n",
    "    \"id\" : 0,\n",
    "    \"name\" : \"Unlicensed\",\n",
    "    \"url\" : \"\",\n",
    "}\n",
    "\n",
    "licenses = [unlicensed_license]\n",
    "images = []\n",
    "annotations = []\n",
    "image_to_annotations = {}\n",
    "nposts = 10000\n",
    "commentsperpost = 5\n",
    "subreddits = [\"rarepuppers\", \"aww\", \"eyebleach\"]\n",
    "now = time.time()\n",
    "rawFolder = \"raw\"\n",
    "\n",
    "captionID = 0 # Initialize to 0\n",
    "for subName in subreddits:\n",
    "    print(\"Extracting data from r/\" + subName + \"...\")\n",
    "    subImages = []\n",
    "    subCaptions = []\n",
    "    for submission in reddit.subreddit(subName).top(\"all\", limit=nposts):\n",
    "        #pprint.pprint(vars(submission))\n",
    "    #     print(submission.comments)\n",
    "        try:\n",
    "            submission.comments.replace_more(limit=0)\n",
    "        except:\n",
    "            continue\n",
    "    #     for top_level_comment in submission.comments[:5]:\n",
    "    #         # Extract first sentence of each top comment.\n",
    "    #         print(re.split('[.!?]',top_level_comment.body)[0]) \n",
    "    \n",
    "        # Filter out gifs, albums\n",
    "        if not submission.url.endswith(('jpg', 'jpeg')):\n",
    "            continue\n",
    "        # Download image and attempt to extract width and height:\n",
    "        filename = submission.id + \".jpg\"\n",
    "        try:\n",
    "            downloadImage(submission.url, os.path.join(rawFolder, filename))\n",
    "        except:\n",
    "            # Just skip this image:\n",
    "            continue\n",
    "        size = get_image_size(os.path.join(rawFolder, filename))\n",
    "        if size is None:\n",
    "            # Manually process image by opening/reading it:\n",
    "            im = Image.open(os.path.join(rawFolder, filename))\n",
    "            size = im.size\n",
    "        width, height = size\n",
    "\n",
    "        # Extract imageID from reddit's base 36\n",
    "        imageID = int(submission.id, 36)\n",
    "\n",
    "        # Generate Image and Annotation json\n",
    "        captions = [\n",
    "            {\n",
    "                \"id\" : captionID + k,\n",
    "                \"image_id\" : imageID,\n",
    "                \"caption\" : get_caption(top_level_comment.body),\n",
    "            }\n",
    "            for k, top_level_comment in enumerate(submission.comments[:commentsperpost])\n",
    "        ]\n",
    "        captionID += commentsperpost\n",
    "        # Include submission title\n",
    "        captions += [\n",
    "            {\n",
    "                \"id\" : captionID,\n",
    "                \"image_id\" : imageID,\n",
    "                \"caption\" : submission.title\n",
    "            }\n",
    "        ]\n",
    "        captionID += 1\n",
    "        \n",
    "        # Add captions and image to lists:\n",
    "        subCaptions += captions\n",
    "        image = {\n",
    "            \"id\" : imageID,\n",
    "            \"width\" : width,\n",
    "            \"height\" : height,\n",
    "            \"file_name\" : filename,\n",
    "            \"license\" : 0,\n",
    "            \"flickr_url\" : \"\",\n",
    "            \"coco_url\" : \"\",\n",
    "            \"date_captured\" : str(date.today()),\n",
    "        }\n",
    "        subImages += [image]\n",
    "        # Keep track of which images get assigned to which captions:\n",
    "        image_to_annotations[imageID] = captions\n",
    "    print(\"\\tRetrieved \"+ str(len(subImages)) + \" images from r/\" + subName + \".\")\n",
    "    print(\"\\tRetrieved \"+ str(len(subCaptions)) + \" captions from r/\" + subName + \".\")\n",
    "    images+= subImages\n",
    "    annotations += subCaptions\n",
    "    \n",
    "# out = {\n",
    "#     \"info\" : info,\n",
    "#     \"images\" : images,\n",
    "#     \"annotations\" : annotations,\n",
    "#     \"licenses\" : licenses\n",
    "# }\n",
    "print(\"Retrieved \"+ str(len(images)) + \" total images.\")\n",
    "print(\"Retrieved \"+ str(len(annotations)) + \" total captions\")\n",
    "print(\"Done. (\" + str(time.time() - now) + \" s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training and validation sets.\n",
      "Training set: \n",
      "\t913 images, 5472 captions.\n",
      "Validation set: \n",
      "\t101 images, 606 captions.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_images = len(images)\n",
    "num_val = num_images/10 #9:1 testing/training ratio\n",
    "# Sample the indices:\n",
    "val_ind = random.sample(range(num_images), num_val)\n",
    "train_ind = [i for i in range(num_images) if i not in val_ind]\n",
    "# Split data into training and evaluation sets:\n",
    "val_images = [images[i] for i in val_ind] \n",
    "val_annotations = []\n",
    "for image in val_images:\n",
    "    val_annotations += image_to_annotations[image[\"id\"]]\n",
    "train_images = [images[i] for i in train_ind ]\n",
    "train_annotations = []\n",
    "for image in train_images:\n",
    "    train_annotations += image_to_annotations[image[\"id\"]]\n",
    "\n",
    "# for i in random.sample(range(num_images), num_val):\n",
    "#     image_id = images[i][\"id\"]\n",
    "#     val_images.append(images[i])\n",
    "#     train_images.remove(images[i])\n",
    "#     # Move images to the correct directory\n",
    "#     filename = images[i][\"file_name\"]\n",
    "\n",
    "print(\"Created training and validation sets.\")\n",
    "print(\"Training set: \")\n",
    "print(\"\\t\"+str(len(train_images))+\" images, \" + str(len(train_annotations))+ \" captions.\")\n",
    "print(\"Validation set: \")\n",
    "print(\"\\t\"+str(len(val_images))+\" images, \" + str(len(val_annotations))+ \" captions.\")     \n",
    "\n",
    "train_json = {\n",
    "    \"info\" : info,\n",
    "    \"images\" : train_images,\n",
    "    \"annotations\" : train_annotations,\n",
    "    \"licenses\" : licenses\n",
    "}\n",
    "\n",
    "val_json = {\n",
    "    \"info\" : info,\n",
    "    \"images\" : val_images,\n",
    "    \"annotations\" : val_annotations,\n",
    "    \"licenses\" : licenses\n",
    "}\n",
    "\n",
    "# print(\"train images:\")\n",
    "# for image in train_images:\n",
    "#     print image[\"id\"]\n",
    "# print(\"val images:\")\n",
    "# for image in val_images:\n",
    "#     print image[\"id\"]\n",
    "    \n",
    "# print(\"train captions:\")\n",
    "# for caption in train_annotations:\n",
    "#     print caption[\"image_id\"]\n",
    "# print(\"val captions:\")\n",
    "# for caption in val_annotations:\n",
    "#     print caption[\"image_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate images into folders and make json files\n",
    "trainFolder = \"reddit_train\"\n",
    "valFolder = \"reddit_val\"\n",
    "for image in train_images:\n",
    "    filename = image[\"file_name\"]\n",
    "    os.rename(os.path.join(rawFolder, filename), os.path.join(trainFolder, filename)) # Move to new location\n",
    "for image in val_images:\n",
    "    filename = image[\"file_name\"]\n",
    "    os.rename(os.path.join(rawFolder, filename), os.path.join(valFolder, filename)) # Move to new location   \n",
    "with open('captions_train.json', 'w') as trainFile:\n",
    "    json.dump(train_json, trainFile)\n",
    "with open('captions_val.json', 'w') as valFile:\n",
    "    json.dump(val_json, valFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot create thumbnail for '5sdon0.jpg'\n",
      "cannot create thumbnail for '51kvfb.jpg'\n",
      "cannot create thumbnail for '5tl4rh.jpg'\n",
      "cannot create thumbnail for '4wklx5.jpg'\n",
      "cannot create thumbnail for '4t90ny.jpg'\n",
      "cannot create thumbnail for '5l9d29.jpg'\n",
      "cannot create thumbnail for '4s6izy.jpg'\n",
      "cannot create thumbnail for '4yg02e.jpg'\n",
      "cannot create thumbnail for '3k3mao.jpg'\n",
      "cannot create thumbnail for '5exgh0.jpg'\n",
      "cannot create thumbnail for '4uo7y4.jpg'\n",
      "cannot create thumbnail for '4xdmjh.jpg'\n",
      "cannot create thumbnail for '2jb8kx.jpg'\n",
      "cannot create thumbnail for '4o5ki1.jpg'\n",
      "cannot create thumbnail for '4msbwt.jpg'\n",
      "cannot create thumbnail for '4eg1ix.jpg'\n",
      "cannot create thumbnail for '4l5mdx.jpg'\n",
      "cannot create thumbnail for '4np4dy.jpg'\n",
      "cannot create thumbnail for '4k2lu3.jpg'\n",
      "cannot create thumbnail for '60jxm8.jpg'\n",
      "cannot create thumbnail for '4mne06.jpg'\n",
      "cannot create thumbnail for '3hg5ap.jpg'\n",
      "cannot create thumbnail for '3rw2qq.jpg'\n",
      "cannot create thumbnail for '4yil2w.jpg'\n",
      "cannot create thumbnail for '5ic5wk.jpg'\n",
      "cannot create thumbnail for '4o12hz.jpg'\n",
      "cannot create thumbnail for '2xj4my.jpg'\n",
      "cannot create thumbnail for '4vmqm2.jpg'\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "rescaledTrainFolder = \"reddit_train_rescaled\"\n",
    "rescaledValFolder = \"reddit_val_rescaled\"\n",
    "rescaled_train_images = []\n",
    "rescaled_train_annotations = []\n",
    "rescaled_val_images = []\n",
    "rescaled_val_annotations = []\n",
    "# Rescale images that are too large:\n",
    "maxsize = (640, 640)\n",
    "for image in train_images:\n",
    "    filename = image[\"file_name\"]\n",
    "    try:\n",
    "        im = Image.open(os.path.join(trainFolder, filename))\n",
    "        im.thumbnail(maxsize, Image.ANTIALIAS)\n",
    "        im.save(os.path.join(rescaledTrainFolder, filename), \"JPEG\")\n",
    "        image[\"width\"], image[\"height\"] = im.size\n",
    "        rescaled_train_images.append(image)\n",
    "        rescaled_train_annotations += image_to_annotations[image[\"id\"]]\n",
    "    except IOError:\n",
    "        print(\"cannot create thumbnail for '%s'\" % filename)\n",
    "        \n",
    "        \n",
    "for image in val_images:\n",
    "    filename = image[\"file_name\"]\n",
    "    try:\n",
    "        im = Image.open(os.path.join(valFolder, filename))\n",
    "        im.thumbnail(maxsize, Image.ANTIALIAS)\n",
    "        im.save(os.path.join(rescaledValFolder, filename), \"JPEG\")\n",
    "        image[\"width\"], image[\"height\"] = im.size\n",
    "        rescaled_val_images.append(image)\n",
    "        rescaled_val_annotations += image_to_annotations[image[\"id\"]]\n",
    "    except IOError:\n",
    "        print(\"cannot create thumbnail for '%s'\" % filename)\n",
    "\n",
    "rescaled_train_json = {\n",
    "    \"info\" : info,\n",
    "    \"images\" : rescaled_train_images,\n",
    "    \"annotations\" : rescaled_train_annotations,\n",
    "    \"licenses\" : licenses\n",
    "}\n",
    "\n",
    "rescaled_val_json = {\n",
    "    \"info\" : info,\n",
    "    \"images\" : rescaled_val_images,\n",
    "    \"annotations\" : rescaled_val_annotations,\n",
    "    \"licenses\" : licenses\n",
    "}\n",
    "\n",
    "with open('rescaled_captions_train.json', 'w') as trainFile:\n",
    "    json.dump(rescaled_train_json, trainFile)\n",
    "with open('rescaled_captions_val.json', 'w') as valFile:\n",
    "    json.dump(rescaled_val_json, valFile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing only: Make small data set\n",
    "\n",
    "from shutil import copyfile\n",
    "smallTrainFolder = \"smalldata_train\"\n",
    "smallValFolder = 'smalldata_val'\n",
    "\n",
    "small_train_annotations = []\n",
    "small_val_annotations = []\n",
    "\n",
    "for image in train_images[:10]:\n",
    "    filename = image[\"file_name\"]\n",
    "    copyfile(os.path.join(trainFolder, filename), os.path.join(smallTrainFolder,filename))\n",
    "    small_train_annotations += image_to_annotations[image[\"id\"]]\n",
    "for image in val_images[:2]:\n",
    "    filename = image[\"file_name\"]\n",
    "    copyfile(os.path.join(valFolder, filename), os.path.join(smallValFolder,filename))\n",
    "    small_val_annotations += image_to_annotations[image[\"id\"]]\n",
    "\n",
    "smalltrain_json = {\n",
    "    \"info\" : info,\n",
    "    \"images\" : train_images[:10],\n",
    "    \"annotations\" : small_train_annotations,\n",
    "    \"licenses\" : licenses\n",
    "}\n",
    "\n",
    "smallval_json = {\n",
    "    \"info\" : info,\n",
    "    \"images\" : val_images[:2],\n",
    "    \"annotations\" : small_val_annotations,\n",
    "    \"licenses\" : licenses\n",
    "}\n",
    "with open('smallcaptions_train.json', 'w') as trainFile:\n",
    "    json.dump(smalltrain_json, trainFile)\n",
    "with open('smallcaptions_val.json', 'w') as valFile:\n",
    "    json.dump(smallval_json, valFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
